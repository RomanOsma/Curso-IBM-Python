import streamlit as st
import requests
import pandas as pd
import json
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langgraph.graph import StateGraph
from langgraph.checkpoint.memory import MemorySaver
import langsmith

# ============================
# üîπ CONFIGURACI√ìN DE BASE DE DATOS VECTORIAL
# ============================
# Se usa Chroma como base de datos vectorial para almacenar y buscar informaci√≥n relacionada con Skyrim.
# HuggingFaceEmbeddings se utiliza para transformar texto en vectores num√©ricos que facilitan la b√∫squeda sem√°ntica.

persist_directory = "db"
embedding_function = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
db = Chroma(persist_directory=persist_directory, collection_name="skyrim-combined", embedding_function=embedding_function)

# ============================
# üîπ CONFIGURACI√ìN DE LM STUDIO (MODELO DE LENGUAJE LOCAL)
# ============================
# LM Studio es un servidor local que permite hacer consultas a modelos de lenguaje (LLMs) sin depender de la nube.

LM_STUDIO_URL = "http://127.0.0.1:1234/v1/chat/completions"
HEADERS = {"Content-Type": "application/json"}

# Funci√≥n para generar respuestas usando el modelo de LM Studio
def generar_respuesta(prompt):
    payload = {
        "model": "deepseek-coder-v2-lite-instruct",
        "messages": [{"role": "user", "content": prompt}]
    }
    
    try:
        response = requests.post(LM_STUDIO_URL, json=payload, headers=HEADERS)
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    except requests.exceptions.RequestException as e:
        return f"‚ö†Ô∏è Error al conectar con LM Studio: {e}"

# ============================
# üîπ INTEGRACI√ìN DE LangGraph PARA MANEJO DE DI√ÅLOGOS
# ============================
# LangGraph nos permite estructurar mejor las respuestas del chatbot definiendo flujos de conversaci√≥n.

# Definimos un estado del chatbot donde guardamos la pregunta y el contexto
class ChatState:
    def __init__(self, pregunta):
        self.pregunta = pregunta
        self.contexto = ""
        self.respuesta = ""

# Funci√≥n para obtener contexto desde la base de datos vectorial
def obtener_contexto(state: ChatState):
    results = db.similarity_search(state.pregunta, k=3)
    state.contexto = "\n".join([r.metadata.get("descripcion", "Sin contexto relevante") for r in results])
    return state

# Funci√≥n para generar respuesta con el modelo de lenguaje
def generar_respuesta_con_contexto(state: ChatState):
    prompt = f"Pregunta: {state.pregunta}\nContexto: {state.contexto}\nRespuesta:"
    state.respuesta = generar_respuesta(prompt)
    return state

# Creaci√≥n del flujo de conversaci√≥n con LangGraph
graph = StateGraph(ChatState)
graph.add_node("obtener_contexto", obtener_contexto)
graph.add_node("generar_respuesta", generar_respuesta_con_contexto)

graph.set_entry_point("obtener_contexto")
graph.add_edge("obtener_contexto", "generar_respuesta")

graph = graph.compile(checkpointer=MemorySaver())

# ============================
# üîπ INTERFAZ DE USUARIO CON STREAMLIT
# ============================
st.title("üîπ Chatbot Skyrim con BD Vectorial y LangGraph")

opcion = st.sidebar.radio("Selecciona una opci√≥n", ["Chat con IA", "Buscar Personajes"])




if opcion == "Chat con IA":
    st.header("üí¨ Habla con el Agente")

    # Configuraci√≥n de la consulta
    consulta = st.text_input("Escribe tu pregunta:")
    col1, col2 = st.columns(2)
    
    with col1:
        fuente_buscada = st.radio(
            "Fuente de b√∫squeda:",
            ("Solo en la base de datos", "B√∫squeda directa con IA")
        )
    
    with col2:
        tipo_buscada = st.radio(
            "Tipo de b√∫squeda vectorial:",
            ("Normal", "Con Score")
        )
    
    if st.button("Enviar"):
        
        if consulta:
            # Si la opci√≥n seleccionada es "Buscar en internet tambi√©n"
            if fuente_buscada == "Buscar en internet tambi√©n":
                # Enviar la consulta a LM Studio para buscar en internet
                if tipo_buscada == "Normal":
                    # Realizar la b√∫squeda de similitud normal
                    results = db.similarity_search(consulta, k=3)  
                    contexto = "\n".join([r.metadata.get("descripcion", "Sin contexto relevante") for r in results]) if results else "Sin contexto relevante"
                else:
                    # Realizar la b√∫squeda de similitud con puntuaciones
                    results = db.similarity_search_with_score(consulta, k=3) 
                    # Aqu√≠ se espera que los resultados sean tuplas, as√≠ que lo manejamos en consecuencia
                    contexto = "\n".join([r[0].metadata.get("descripcion", "Sin contexto relevante") for r in results]) if results else "Sin contexto relevante"
                    
                # Enviar la consulta y contexto a LM Studio (solo con la base de datos)
                prompt = f"Pregunta: {consulta}\nContexto: {contexto}\nRespuesta:"
                respuesta = generar_respuesta(prompt)
                
                #state = ChatState(consulta)
                #state = graph.invoke(state)

                st.write("üß† Respuesta de la IA (Internet + BD):")
                st.write(respuesta)
            else:
                # Buscar solo en la base de datos vectorial
                if tipo_buscada == "Normal":
                    # Realizar la b√∫squeda de similitud normal
                    results = db.similarity_search(consulta, k=3)  
                    contexto = "\n".join([r.metadata.get("descripcion", "Sin contexto relevante") for r in results]) if results else "Sin contexto relevante"
                else:
                    # Realizar la b√∫squeda de similitud con puntuaciones
                    results = db.similarity_search_with_score(consulta, k=3) 
                    # Aqu√≠ se espera que los resultados sean tuplas, as√≠ que lo manejamos en consecuencia
                    contexto = "\n".join([r[0].metadata.get("descripcion", "Sin contexto relevante") for r in results]) if results else "Sin contexto relevante"
                    
                # Enviar la consulta y contexto a LM Studio (solo con la base de datos)
                prompt = f"Pregunta: {consulta}\nContexto: {contexto}\nRespuesta:"
                respuesta = generar_respuesta(prompt)

                #state = ChatState(consulta)
                #state = graph.invoke(state)

                st.write("üß† Respuesta de la IA (Solo BD Vectorial):")
                st.write(respuesta)


elif opcion == "Buscar Personajes":
    st.header("üîç Filtrar Personajes")
    
    # Cargar datos de personajes
    csv_file = "data/Skyrim_Named_Characters_Propio.csv"
    df = pd.read_csv(csv_file)
    
    # Filtrar opciones
    raza_opciones = ["Todas"] + sorted(df['Race'].unique().tolist())
    genero_opciones = ["Todos"] + sorted(df['Gender'].unique().tolist())
    clase_opciones = ["Todas"] + sorted(df['Class'].unique().tolist())
    
    raza = st.selectbox("Selecciona una raza", raza_opciones)
    genero = st.selectbox("Selecciona un g√©nero", genero_opciones)
    clase = st.selectbox("Selecciona una clase", clase_opciones)

    # Filtrar personajes
    filtered_df = df.copy()
    if raza != "Todas":
        filtered_df = filtered_df[filtered_df['Race'] == raza]
    if genero != "Todos":
        filtered_df = filtered_df[filtered_df['Gender'] == genero]
    if clase != "Todas":
        filtered_df = filtered_df[filtered_df['Class'] == clase]

    st.write("### Resultados de la b√∫squeda:")
    
    if not filtered_df.empty:
        selected_personaje_name = st.selectbox("Selecciona un personaje", filtered_df['Name'].tolist())
        selected_personaje = filtered_df[filtered_df['Name'] == selected_personaje_name].iloc[0]

        # Mostrar la tabla de resultados
        filtered_df_display = filtered_df[['Name', 'Home City', 'House', 'Race', 'Gender', 'Level', 'Class', 'Primary Skills', 'Faction(s)', 'Location']]

        # Convertir la tabla a una tabla interactiva con Streamlit
        selected_rows = st.dataframe(filtered_df_display)
        
        # Obtener los datos del personaje seleccionado
        selected_personaje = filtered_df[filtered_df['Name'] == selected_personaje_name].iloc[0]

        personaje_name = selected_personaje['Name']
        
        # Cargar im√°genes y enlaces de wiki desde JSON
        json_file = "data/skyrim_images.json"
        with open(json_file) as f:
            images_data = json.load(f)
        
        wiki_link = None
        image_link = None
        for item in images_data:
            if item["Name"] == personaje_name:
                wiki_link = item["Source"]
                image_link = item["Image"]
                break
        
        if wiki_link and image_link:
            st.markdown(f"### Detalles del Personaje: {personaje_name}")
            st.markdown(f"üîó [Wiki - {personaje_name}]({wiki_link})")
            st.image(image_link, caption=personaje_name, use_container_width=True)
    else:
        st.write("‚ö†Ô∏è No se encontraron personajes con esos filtros.")
