{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba30eb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las bibliotecas necesarias para el funcionamiento del script.\n",
    "# `requests`: Se utiliza para realizar solicitudes HTTP y obtener el contenido de páginas web.\n",
    "# `BeautifulSoup` (importado como `bs`): Es una herramienta poderosa para parsear y manipular documentos HTML/XML.\n",
    "# `json`: Permite trabajar con datos en formato JSON, como guardar y cargar archivos estructurados.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "57063b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la función `get_information`, que extrae información de una página wiki específica.\n",
    "# Argumentos:\n",
    "# - `full_path`: La URL completa de la página wiki que queremos analizar.\n",
    "def get_information(full_path):\n",
    "  try:\n",
    "    # Realizamos una solicitud GET a la URL proporcionada (`full_path`) usando la biblioteca `requests`.\n",
    "    r = requests.get(full_path)\n",
    "    \n",
    "    # Convertimos el contenido de la respuesta en un objeto BeautifulSoup.\n",
    "    soup = bs(r.content, 'html.parser')\n",
    "    \n",
    "    # Buscamos un elemento con la clase `wikitable infobox`. Este elemento típicamente contiene información estructurada sobre un tema en Wikipedia.\n",
    "    info_box = soup.find(class_='wikitable infobox')\n",
    "    if not info_box:\n",
    "      print(f\"No infobox found on page: {full_path}\")\n",
    "      return None  # Devolvemos None si no hay información estructurada\n",
    "    \n",
    "    # Extraemos todas las filas (`tr`) dentro del cuadro de información.\n",
    "    info_rows = info_box.find_all('tr')\n",
    "    \n",
    "    # Creamos un diccionario vacío llamado `character_info` para almacenar los datos extraídos.\n",
    "    character_info = {}\n",
    "    character_info['Source'] = full_path  # Agregamos la fuente (la URL de la página).\n",
    "    \n",
    "    for index, row in enumerate(info_rows):\n",
    "      if index == 0:\n",
    "        # La primera fila suele contener el nombre principal del personaje o tema.\n",
    "        character_info['Name'] = row.find('th').get_text().split('\\n')[0]\n",
    "      \n",
    "      elif row.find('td') is None:\n",
    "        # Si la fila no contiene celdas de datos (`td`), usamos el texto del encabezado como clave con valor True.\n",
    "        content_key = row.find('th').get_text()\n",
    "        character_info[content_key] = True\n",
    "      \n",
    "      else:\n",
    "        # Procesamos otras filas según su contenido.\n",
    "        if (len(row.find_all('th')) and len(row.find_all('td'))) > 1:\n",
    "          # Si la fila contiene múltiples encabezados (`th`) y múltiples celdas de datos (`td`),\n",
    "          # iteramos sobre ellos y los agregamos al diccionario `character_info`.\n",
    "          content_keys = row.find_all('th')\n",
    "          content_values = row.find_all('td')\n",
    "          for key, value in zip(content_keys, content_values):\n",
    "            character_info[key.get_text()] = value.get_text()\n",
    "        \n",
    "        elif row.find_all('img'):\n",
    "          # Si la fila contiene imágenes (`img`), tratamos de extraer información relevante de ellas.\n",
    "          content_key = row.find('th').get_text()\n",
    "          content_value = ''\n",
    "          for i, img in enumerate(row.select('img')):\n",
    "            if i == 1:  # Solo seleccionamos la segunda imagen.\n",
    "              content_value = img['alt']\n",
    "          character_info[content_key] = content_value\n",
    "        \n",
    "        elif len(row.select('td a')) > 1:\n",
    "          # Si la fila contiene múltiples enlaces (`a`) dentro de una celda de datos (`td`),\n",
    "          # concatenamos todos los textos de las celdas separándolos por comas.\n",
    "          content_key = row.find('th').get_text()\n",
    "          content_value = row.find('td').get_text().replace('\\n', ', ')\n",
    "          character_info[content_key] = content_value\n",
    "        \n",
    "        else:\n",
    "          # Para todos los demás casos, simplemente extraemos el texto del encabezado (`th`) y el texto de la celda de datos (`td`).\n",
    "          content_key = row.find('th').get_text()\n",
    "          content_value = row.find('td').get_text()\n",
    "          character_info[content_key] = content_value\n",
    "    \n",
    "    # Finalmente, devolvemos el diccionario `character_info` con todos los datos extraídos.\n",
    "    return character_info\n",
    "  \n",
    "  except Exception as e:\n",
    "    # Si ocurre algún error durante el proceso, mostramos un mensaje de advertencia con detalles del error.\n",
    "    print(f'Error processing page {full_path}: {e}')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58da56ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la función `save_data`, que guarda los datos en un archivo JSON.\n",
    "# Argumentos:\n",
    "# - `title`: El nombre del archivo donde se guardarán los datos.\n",
    "# - `data`: Los datos que queremos guardar. Estos deben ser serializables a JSON.\n",
    "def save_data(title, data):\n",
    "  with open(title, 'w', encoding='utf-8') as f:\n",
    "    # La función `json.dump()` convierte el objeto Python `data` en una cadena JSON y lo guarda en el archivo `f`.\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Definimos la función `load_data`, que carga datos desde un archivo JSON.\n",
    "# Argumentos:\n",
    "# - `title`: El nombre del archivo JSON desde donde se cargarán los datos.\n",
    "def load_data(title):\n",
    "  with open(title, encoding='utf-8') as f:\n",
    "    # La función `json.load()` lee el contenido del archivo `f` y lo convierte en un objeto Python (por ejemplo, una lista o diccionario).\n",
    "    return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b8a6e287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la URL base del sitio web. Esta será usada para construir las URLs completas a partir de enlaces relativos.\n",
    "# La URL base es comúnmente utilizada cuando los enlaces en una página son relativos (es decir, no incluyen el dominio completo).\n",
    "base_link = 'https://en.uesp.net'\n",
    "\n",
    "# Realizamos una solicitud GET a la URL proporcionada ('https://en.uesp.net/wiki/Skyrim:People').\n",
    "# El objeto `r` contiene la respuesta de la solicitud, incluyendo el contenido HTML de la página.\n",
    "# En este caso, estamos accediendo a una página que contiene información sobre los personajes del juego \"Skyrim\".\n",
    "# Ejemplo: Esta línea obtendrá todo el contenido HTML de la página donde se listan las personas en Skyrim.\n",
    "r = requests.get('https://en.uesp.net/wiki/Skyrim:People')\n",
    "\n",
    "# Convertimos el contenido de la respuesta en un objeto BeautifulSoup.\n",
    "# Esto nos permite trabajar con el HTML de manera más estructurada y fácil de manipular.\n",
    "# Usamos 'html.parser' como motor de análisis, que es un parser incluido con Python por defecto.\n",
    "# Este paso es crucial porque convierte el contenido HTML en un formato que podemos navegar y buscar fácilmente.\n",
    "soup = bs(r.content, 'html.parser')\n",
    "\n",
    "# Utilizamos el método `select` de BeautifulSoup para encontrar todos los elementos que cumplen con el selector CSS especificado.\n",
    "# `.wikitable tr b > a`: Este selector busca dentro de una tabla con la clase `wikitable`, todas las filas (`tr`), luego dentro de cada fila busca etiquetas `<b>` y finalmente selecciona los enlaces (`<a>`) directamente dentro de estas etiquetas `<b>`.\n",
    "# Este selector específico está diseñado para extraer los enlaces de los nombres de los personajes en la tabla de personas de Skyrim.\n",
    "# Ejemplo: Si hay una fila con `<tr><td><b><a href=\"/wiki/Character1\">Character1</a></b></td></tr>`, este selector capturará el enlace `/wiki/Character1`.\n",
    "people_list = soup.select('.wikitable tr b > a')\n",
    "\n",
    "# Explicación detallada de cómo funciona el selector CSS:\n",
    "# - `.wikitable`: Busca una tabla con la clase `wikitable`. Es común que las tablas en sitios wiki tengan esta clase.\n",
    "# - `tr`: Dentro de esa tabla, busca todas las filas (`tr`).\n",
    "# - `b > a`: Dentro de cada fila, busca etiquetas `<b>` que contienen directamente un enlace `<a>`.\n",
    "# Este selector asegura que solo obtenemos los enlaces relacionados con los nombres de los personajes, ignorando otros enlaces o elementos irrelevantes.\n",
    "\n",
    "# Resultado esperado:\n",
    "# La variable `people_list` ahora contiene una lista de objetos `Tag` de BeautifulSoup, donde cada objeto representa un enlace (`<a>`) encontrado.\n",
    "# Cada elemento de esta lista puede ser usado para extraer atributos como `href` (la URL del enlace) o el texto visible del enlace.\n",
    "# Ejemplo: Si `people_list[0]` es `<a href=\"/wiki/Character1\">Character1</a>`, entonces:\n",
    "# - `people_list[0].get('href')` devolverá \"/wiki/Character1\".\n",
    "# - `people_list[0].get_text()` devolverá \"Character1\".\n",
    "\n",
    "# Nota: Para ejecutar este código correctamente, asegúrate de tener instaladas las bibliotecas `requests` y `beautifulsoup4`.\n",
    "# Puedes instalarlas usando pip:\n",
    "# pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "edffca86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping forbidden URL: https://en.uesp.net/wiki/Skyrim:Followers\n",
      "Skipping forbidden URL: https://en.uesp.net/wiki/Skyrim:Merchants\n",
      "Skipping forbidden URL: https://en.uesp.net/wiki/Skyrim:Trainers\n",
      "Skipping forbidden URL: https://en.uesp.net/wiki/Skyrim:Dark_Brotherhood_Initiate\n",
      "Skipping forbidden URL: https://en.uesp.net/wiki/Skyrim:Followers\n",
      "No infobox found on page: https://en.uesp.net/wiki/Skyrim:Night_Mother\n",
      "No infobox found on page: https://en.uesp.net/wiki/Skyrim:Night_Mother\n",
      "Skipping forbidden URL: https://en.uesp.net/wiki/Skyrim:Followers\n",
      "No infobox found on page: https://en.uesp.net/wiki/Skyrim:Orc\n",
      "No infobox found on page: https://en.uesp.net/wiki/Skyrim:Armored_Troll\n",
      "Skipping forbidden URL: https://en.uesp.net/wiki/Skyrim:Followers\n",
      "Skipping invalid link: https://en.uesp.net/w/index.php?title=Skyrim:Imperial_Soldier_(Sovngarde)&action=edit&redlink=1\n"
     ]
    }
   ],
   "source": [
    "# Scrape the data based on the links\n",
    "# Iniciamos una lista vacía llamada `characters_data` que almacenará los datos de cada personaje obtenidos del scraping.\n",
    "# Esta lista actuará como un contenedor para todos los diccionarios con información de los personajes.\n",
    "characters_data = []\n",
    "\n",
    "# Iteramos sobre la lista `people_list`, que se supone que contiene objetos `Tag` de BeautifulSoup representando enlaces a páginas de personajes.\n",
    "# Usamos `enumerate` para tener acceso al índice (`index`) y al elemento actual (`people`) en cada iteración.\n",
    "# Ejemplo: Si `people_list` contiene [<a href=\"/wiki/Character1\">Character1</a>, <a href=\"/wiki/Character2\">Character2</a>], \n",
    "# entonces en la primera iteración `people` sería `<a href=\"/wiki/Character1\">Character1</a>` y `index` sería 0.\n",
    "for index, people in enumerate(people_list):\n",
    "  try:\n",
    "    # Obtenemos el enlace relativo usando el atributo `href` del objeto `people`.\n",
    "    # Usamos `.get('href', '')` para evitar errores si el atributo `href` no existe.\n",
    "    relative_link = people.get('href', '')\n",
    "    \n",
    "    # Si el enlace relativo está vacío, lo omitimos e imprimimos un mensaje de advertencia.\n",
    "    if not relative_link:\n",
    "      print(f\"Skipping empty link at index {index}\")\n",
    "      continue\n",
    "    \n",
    "    # Construimos la URL completa concatenando la URL base (`base_link`) con el enlace relativo (`relative_link`).\n",
    "    fullpath = base_link + relative_link\n",
    "    \n",
    "    # Filtramos enlaces no relevantes (categorías o listas).\n",
    "    # Si el texto del enlace contiene palabras clave como \"Followers\", \"Merchants\" o \"Trainers\", lo omitimos.\n",
    "    # Filtramos URLs específicas que no queremos procesar.\n",
    "    if any(forbidden_url in fullpath for forbidden_url in ['Dark_Brotherhood_Initiate', 'Followers', 'Merchants', 'Trainers']):\n",
    "      print(f\"Skipping forbidden URL: {fullpath}\")\n",
    "      continue\n",
    "    \n",
    "    # Verificamos si el enlace apunta a una redirección o página inválida.\n",
    "    # Si el enlace contiene 'redlink=1' o está vacío después de eliminar espacios, lo omitimos.\n",
    "    if 'redlink=1' in fullpath or not relative_link.strip():\n",
    "      print(f\"Skipping invalid link: {fullpath}\")\n",
    "      continue\n",
    "    \n",
    "    # Procesamos solo enlaces válidos llamando a la función `get_information`.\n",
    "    data = get_information(fullpath)\n",
    "    \n",
    "    # Si `get_information` devuelve datos válidos, los añadimos a la lista `characters_data`.\n",
    "    if data:\n",
    "      characters_data.append(data)\n",
    "  \n",
    "  except Exception as e:\n",
    "    # Si ocurre algún error durante el procesamiento de un enlace, mostramos un mensaje de advertencia con detalles del error.\n",
    "    print(f'WARNING: Failed to process {people.get_text()} {fullpath} - Error: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "14902e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters processed: 1062\n"
     ]
    }
   ],
   "source": [
    "# Guardamos los datos recopilados previamente (`characters_data`) en un archivo llamado 'skyrim_population.json'.\n",
    "# La función `save_data` toma el nombre del archivo ('skyrim_population.json') y los datos (`characters_data`) como argumentos.\n",
    "# Ejemplo: Si `characters_data` contiene [{'Name': 'Alduin', 'Race': 'Dragon'}, {'Name': 'Ulfric', 'Race': 'Human'}],\n",
    "# este código creará un archivo JSON con estos datos estructurados.\n",
    "save_data('skyrim_population.json', characters_data)\n",
    "\n",
    "# Explicación adicional:\n",
    "# - La función `save_data` es útil para almacenar grandes volúmenes de datos estructurados en un archivo persistente.\n",
    "# - La función `load_data` permite recuperar esos datos más tarde sin necesidad de volver a realizar el scraping.\n",
    "# - Ambas funciones utilizan la codificación UTF-8 para garantizar que todos los caracteres, incluso aquellos fuera del alfabeto inglés, sean manejados correctamente.\n",
    "\n",
    "# Ejemplo de uso de `load_data`:\n",
    "# Supongamos que hemos guardado los datos anteriormente y ahora queremos cargarlos:\n",
    "# loaded_data = load_data('skyrim_population.json')\n",
    "# print(loaded_data)  # Esto imprimirá los datos cargados desde el archivo JSON.\n",
    "# print({len(loaded_data)})  # Esto imprimirá el numero de datos cargados desde el archivo JSON.\n",
    "print(f\"Total characters processed: {len(characters_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9c9e6cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la función `extract_image_url`, que extrae la URL de la imagen desde una página wiki.\n",
    "# Argumentos:\n",
    "# - `full_path`: La URL completa de la página wiki que queremos analizar.\n",
    "def extract_image_url(full_path):\n",
    "  try:\n",
    "    # Realizamos una solicitud GET a la URL proporcionada (`full_path`) usando la biblioteca `requests`.\n",
    "    r = requests.get(full_path)\n",
    "    \n",
    "    # Convertimos el contenido de la respuesta en un objeto BeautifulSoup.\n",
    "    soup = bs(r.content, 'html.parser')\n",
    "    \n",
    "    # Buscamos el elemento <meta property=\"og:image\"> que contiene la URL de la imagen.\n",
    "    meta_tag = soup.find('meta', property='og:image')\n",
    "    \n",
    "    # Si encontramos el elemento, devolvemos su atributo 'content' (la URL de la imagen).\n",
    "    if meta_tag and 'content' in meta_tag.attrs:\n",
    "      return meta_tag['content']\n",
    "    \n",
    "    # Si no se encuentra ninguna imagen, mostramos un mensaje de advertencia y devolvemos `None`.\n",
    "    print(f\"No image found for page: {full_path}\")\n",
    "    return None\n",
    "  \n",
    "  except Exception as e:\n",
    "    # Si ocurre algún error durante el procesamiento, mostramos un mensaje de advertencia con detalles del error.\n",
    "    print(f\"Error extracting image from {full_path}: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4ceaa07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la función `save_data` que se encarga de guardar datos en un archivo JSON.\n",
    "# Argumentos:\n",
    "# - `title`: El nombre del archivo donde se guardarán los datos (por ejemplo, 'skyrim_population.json').\n",
    "# - `data`: Los datos que queremos guardar. Estos deben ser serializables a JSON (por ejemplo, listas, diccionarios, etc.).\n",
    "def save_data(title, data):\n",
    "  # Usamos el contexto `with open()` para abrir el archivo especificado por `title` en modo escritura (`'w'`).\n",
    "  # El argumento `encoding='utf-8'` asegura que los caracteres especiales (como tildes o letras no latinas) sean correctamente guardados.\n",
    "  with open(title, 'w', encoding='utf-8') as f:\n",
    "    # La función `json.dump()` convierte el objeto Python `data` en una cadena JSON y lo guarda en el archivo `f`.\n",
    "    # - `ensure_ascii=False`: Permite que los caracteres no ASCII (como tildes o acentos) sean almacenados directamente, en lugar de ser escapados.\n",
    "    # - `indent=2`: Formatea el archivo JSON con sangría de 2 espacios para mejorar su legibilidad.\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "36a1498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos previamente extraídos desde el archivo `skyrim_population.json`.\n",
    "try:\n",
    "  with open('skyrim_population.json', encoding='utf-8') as f:\n",
    "    characters_data = json.load(f)  # Cargamos los datos como una lista de diccionarios.\n",
    "except FileNotFoundError:\n",
    "  print(\"El archivo skyrim_population.json no fue encontrado.\")\n",
    "  characters_data = []  # Si el archivo no existe, inicializamos una lista vacía."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b38641e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una lista vacía llamada `characters_images` para almacenar los nuevos datos (Source, Name e Image).\n",
    "characters_images = []\n",
    "\n",
    "# Procesamos cada entrada en `characters_data`.\n",
    "for character in characters_data:\n",
    "  try:\n",
    "    # Obtenemos la URL del personaje (clave 'Source') y su nombre (clave 'Name').\n",
    "    source_url = character.get('Source')\n",
    "    name = character.get('Name', 'Unknown')  # Usamos 'Unknown' si no hay nombre.\n",
    "    \n",
    "    # Verificamos si la URL existe.\n",
    "    if not source_url:\n",
    "      print(f\"Skipping entry without a valid source URL for character: {name}\")\n",
    "      continue\n",
    "    \n",
    "    # Extraemos la URL de la imagen usando la función `extract_image_url`.\n",
    "    image_url = extract_image_url(source_url)\n",
    "    \n",
    "    # Si se encontró una imagen, la añadimos a la lista junto con la URL del personaje y su nombre.\n",
    "    if image_url:\n",
    "      characters_images.append({\n",
    "        'Source': source_url,\n",
    "        'Name': name,\n",
    "        'Image': image_url\n",
    "      })\n",
    "  \n",
    "  except Exception as e:\n",
    "    # Si ocurre algún error durante el procesamiento, mostramos un mensaje de advertencia con detalles del error.\n",
    "    print(f\"Error processing character {character.get('Name', 'Unknown')} - Source: {source_url} - Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "38e8f0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters-images processed: 1062\n"
     ]
    }
   ],
   "source": [
    "# Guardamos los datos resultantes en un nuevo archivo JSON llamado `skyrim_images.json`.\n",
    "save_data('skyrim_images.json', characters_images)\n",
    "\n",
    "# Mostramos el número total de entradas procesadas correctamente.\n",
    "print(f\"Total characters-images processed: {len(characters_images)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
